{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "import en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/jovyan/Documents/MSBA/unstrucured_data/mgta-415-winter2024/\"\n",
    "\n",
    "df_train = pd.read_csv(data_path + \"train.csv\")\n",
    "df_test = pd.read_csv(data_path + \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Example usage:\\ndf['processed_text'] = df['text'].apply(lambda x: ' '.                                        join(combined_preprocess(x, stemming=True, need_sent=False, use_spacy=True)))\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def preprocess(doc, stemming=True, need_sent=False, use_spacy=False):\n",
    "    # Step 0: Prepare resources\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('would')\n",
    "    ps = PorterStemmer()\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    \n",
    "    # Step 1: Lowercase and remove punctuation\n",
    "    doc = doc.lower().translate(translator)\n",
    "    \n",
    "    # Step 2 & 3: Tokenize into sentences, then words, and apply stemming if needed\n",
    "    sentences = sent_tokenize(doc)\n",
    "    processed_tokens = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        if stemming:\n",
    "            words = [ps.stem(word) for word in words]\n",
    "        words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "        if need_sent:\n",
    "            processed_tokens.append(words)\n",
    "        else:\n",
    "            processed_tokens.extend(words)\n",
    "    \n",
    "    # Optional: Use spaCy for further processing (like lemmatization or entity recognition)\n",
    "    if use_spacy:\n",
    "        # Since spaCy expects a string, join tokens for spaCy processing and then re-tokenize if needed\n",
    "        spacy_doc = nlp(\" \".join(processed_tokens))\n",
    "        # Example: Replace processed_tokens with lemma. Modify as needed for your use case\n",
    "        processed_tokens = [token.lemma_ for token in spacy_doc]\n",
    "        if need_sent:\n",
    "            # If sentence structure was required, this needs rethinking based on spaCy's output\n",
    "            raise NotImplementedError(\"Combining spaCy processing with sentence-wise token output not directly supported.\")\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "\"\"\"\n",
    "# Example usage:\n",
    "df['processed_text'] = df['text'].apply(lambda x: ' '.\\\n",
    "                                        join(combined_preprocess(x, stemming=True, need_sent=False, use_spacy=True)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] =df_train['review'].apply(lambda x: ' '.\\\n",
    "                                        join(preprocess(x, stemming=True, need_sent=False, use_spacy=False)))\n",
    "\n",
    "df_test['text'] =df_test['review'].apply(lambda x: ' '.\\\n",
    "                                        join(preprocess(x, stemming=True, need_sent=False, use_spacy=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the preprocessed DataFrame to a Pickle file\n",
    "with open('df_train_preprocessed.pkl', 'wb') as file:\n",
    "    pickle.dump(df_train, file)\n",
    "\n",
    "with open('df_test_preprocessed.pkl', 'wb') as file:\n",
    "    pickle.dump(df_test, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
