{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/jovyan/git/machine_learning/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed DataFrame from a Pickle file\n",
    "with open('df_train_preprocessed.pkl', 'rb') as file:\n",
    "    df_train = pickle.load(file)\n",
    "\n",
    "with open('df_test_preprocessed.pkl', 'rb') as file:\n",
    "    df_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(inp_data, vocabulary_inv, size_features=150,\n",
    "                   mode='skipgram',\n",
    "                   min_word_count=2,\n",
    "                   context=9):\n",
    "    model_name = \"embedding\"\n",
    "    model_name = os.path.join(model_name)\n",
    "    num_workers = 15  # Number of threads to run in parallel\n",
    "    downsampling = 1e-3  # Downsample setting for frequent words\n",
    "    print('Training Word2Vec model...')\n",
    "    # use inp_data and vocabulary_inv to reconstruct sentences\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "    if mode == 'skipgram':\n",
    "        sg = 1\n",
    "        print('Model: skip-gram')\n",
    "    elif mode == 'cbow':\n",
    "        sg = 0\n",
    "        print('Model: CBOW')\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                        sg=sg,\n",
    "                                        vector_size=size_features,\n",
    "                                        min_count=min_word_count,\n",
    "                                        window=context,\n",
    "                                        sample=downsampling)\n",
    "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "    for i in range(len(vocabulary_inv)):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model.wv:\n",
    "            embedding_weights[i] = embedding_model.wv[word]\n",
    "        else:\n",
    "            embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
    "                                                     embedding_model.vector_size)\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "# build vocabulary from tokenized data\n",
    "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
    "# use the above mapping to create input data\n",
    "inp_data = [[vocabulary[word] for word in text] for text in tagged_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n"
     ]
    }
   ],
   "source": [
    "embedding_weights_2vec = get_embeddings(inp_data, vocabulary_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(df_test[\"text\"])]\n",
    "\n",
    "train_vec = []\n",
    "for doc in tagged_train_data:\n",
    "    vec = 0\n",
    "    for w in doc:\n",
    "        vec += embedding_weights_2vec [vocabulary[w]]\n",
    "    vec = vec / len(doc)\n",
    "    train_vec.append(vec)\n",
    "\n",
    "import random\n",
    "labels = df_train[\"label\"].tolist()\n",
    "combined = list(zip(train_vec, labels))\n",
    "\n",
    "random.shuffle(combined)\n",
    "shuffled_train_vec, shuffled_labels = zip(*combined)\n",
    "split_size = int(len(shuffled_train_vec) * 0.8)\n",
    "train_vec1 = shuffled_train_vec[:split_size]\n",
    "valid_vec1 = shuffled_train_vec[split_size:]\n",
    "\n",
    "train_labels1 = shuffled_labels[:split_size]\n",
    "valid_labels1 = shuffled_labels[split_size:]\n",
    "\n",
    "test_vec = []\n",
    "for doc in tagged_test_data:\n",
    "    vec = 0\n",
    "    length = 0\n",
    "    for w in doc:\n",
    "        try:\n",
    "            vec += embedding_weights_2vec [vocabulary[w]]\n",
    "            length += 1\n",
    "        except:\n",
    "            continue\n",
    "    vec = vec / length\n",
    "    test_vec.append(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logistic_regression', LogisticRegression(\n",
    "        penalty='l1', \n",
    "        C=1,  \n",
    "        solver='saga', \n",
    "        max_iter=10000\n",
    "    ))\n",
    "])\n",
    "\n",
    "clf = logistic_regression_pipeline.fit(train_vec1, train_labels1)\n",
    "preds_valid = clf.predict(valid_vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "xgboost_pipeline = Pipeline([\n",
    "    ('xgboost', XGBClassifier(\n",
    "        objective='multi:softmax',  \n",
    "        num_class=10,  \n",
    "        eval_metric='mlogloss',  \n",
    "        use_label_encoder=False,  \n",
    "        learning_rate=0.1,  \n",
    "        n_estimators=100,  \n",
    "        max_depth=6,  \n",
    "        seed=42  \n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels1)\n",
    "valid_labels_encoded = label_encoder.transform(valid_labels1) \n",
    "\n",
    "clf = xgboost_pipeline.fit(train_vec1, train_labels_encoded)\n",
    "preds_valid_encoded = clf.predict(valid_vec1)\n",
    "\n",
    "preds_valid = label_encoder.inverse_transform(preds_valid_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7923164701407379\n",
      "AUC Score: 0.9630866553140578\n",
      "F1 Score (Macro): 0.7268754524809894\n",
      "F1 Score (Micro): 0.7923164701407379\n",
      "F1 Score (Weighted): 0.783039664084486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(valid_labels1, preds_valid)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "pred_probs = clf.predict_proba(valid_vec1)\n",
    "auc_score = roc_auc_score(valid_labels1, pred_probs, multi_class='ovr')\n",
    "print(f'AUC Score: {auc_score}')\n",
    "\n",
    "f1_macro = f1_score(valid_labels1, preds_valid, average='macro')\n",
    "f1_micro = f1_score(valid_labels1, preds_valid, average='micro')\n",
    "f1_weighted = f1_score(valid_labels1, preds_valid, average='weighted')\n",
    "\n",
    "print(f'F1 Score (Macro): {f1_macro}')\n",
    "print(f'F1 Score (Micro): {f1_micro}')\n",
    "print(f'F1 Score (Weighted): {f1_weighted}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial1: Using size_features=150, C = 100 , the accuracy score is 0.779\n",
    "\n",
    "Trial2: Using size_features=150, C = 1 , the accuracy score is 0.784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the whole training set and predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = logistic_regression_pipeline.fit(train_vec, df_train[\"label\"])\n",
    "preds = clf.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in your implemetation, create the output file using the same format\n",
    "dic = {\"Id\": [], \"Predicted\": []}\n",
    "for i, pred in enumerate(preds):\n",
    "    dic[\"Id\"].append(i)\n",
    "    dic[\"Predicted\"].append(pred)\n",
    "\n",
    "dic_df = pd.DataFrame.from_dict(dic)\n",
    "dic_df.to_csv(data_path + \"predicted_Cl2saga_3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
