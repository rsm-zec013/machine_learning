{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import re\n",
    "import en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/jovyan/Documents/MSBA/unstrucured_data/mgta-415-winter2024/\"\n",
    "\n",
    "df_train = pd.read_csv(data_path + \"train.csv\")\n",
    "df_test = pd.read_csv(data_path + \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Example usage:\\ndf['processed_text'] = df['text'].apply(lambda x: ' '.                                        join(combined_preprocess(x, stemming=True, need_sent=False, use_spacy=True)))\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def preprocess(doc, stemming=True, need_sent=False, use_spacy=False):\n",
    "    # Step 0: Prepare resources\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('would')\n",
    "    ps = PorterStemmer()\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    \n",
    "    # Step 1: Lowercase and remove punctuation\n",
    "    doc = doc.lower().translate(translator)\n",
    "    \n",
    "    # Step 2 & 3: Tokenize into sentences, then words, and apply stemming if needed\n",
    "    sentences = sent_tokenize(doc)\n",
    "    processed_tokens = []\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        if stemming:\n",
    "            words = [ps.stem(word) for word in words]\n",
    "        words = [word for word in words if word not in stop_words and len(word) > 1]\n",
    "        if need_sent:\n",
    "            processed_tokens.append(words)\n",
    "        else:\n",
    "            processed_tokens.extend(words)\n",
    "    \n",
    "    # Optional: Use spaCy for further processing (like lemmatization or entity recognition)\n",
    "    if use_spacy:\n",
    "        # Since spaCy expects a string, join tokens for spaCy processing and then re-tokenize if needed\n",
    "        spacy_doc = nlp(\" \".join(processed_tokens))\n",
    "        # Example: Replace processed_tokens with lemma. Modify as needed for your use case\n",
    "        processed_tokens = [token.lemma_ for token in spacy_doc]\n",
    "        if need_sent:\n",
    "            # If sentence structure was required, this needs rethinking based on spaCy's output\n",
    "            raise NotImplementedError(\"Combining spaCy processing with sentence-wise token output not directly supported.\")\n",
    "    \n",
    "    return processed_tokens\n",
    "\n",
    "\"\"\"\n",
    "# Example usage:\n",
    "df['processed_text'] = df['text'].apply(lambda x: ' '.\\\n",
    "                                        join(combined_preprocess(x, stemming=True, need_sent=False, use_spacy=True)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] =df_train['review'].apply(lambda x: ' '.\\\n",
    "                                        join(preprocess(x, stemming=True, need_sent=False, use_spacy=False)))\n",
    "\n",
    "df_test['text'] =df_test['review'].apply(lambda x: ' '.\\\n",
    "                                        join(preprocess(x, stemming=True, need_sent=False, use_spacy=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the preprocessed DataFrame to a Pickle file\n",
    "with open('df_train_preprocessed.pkl', 'wb') as file:\n",
    "    pickle.dump(df_train, file)\n",
    "\n",
    "with open('df_test_preprocessed.pkl', 'wb') as file:\n",
    "    pickle.dump(df_test, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "# build vocabulary from tokenized data\n",
    "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
    "# use the above mapping to create input data\n",
    "inp_data = [[vocabulary[word] for word in text] for text in tagged_data]\n",
    "\n",
    "# get embedding vector\n",
    "#embedding_weights = get_embeddings(inp_data, vocabulary_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vocabulary = 10000\n",
    "tokenizer = Tokenizer(num_words=vocabulary)\n",
    "\n",
    "# Tokenize the preprocessed texts\n",
    "texts_train = df_train['text'].tolist()\n",
    "texts_test = df_test['text'].tolist()\n",
    "\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequence_train = tokenizer.texts_to_sequences(texts_train)\n",
    "sequence_test = tokenizer.texts_to_sequences(texts_test)\n",
    "\n",
    "# Define the maximum number of words in a sequence\n",
    "word_num = 350\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "x_train = pad_sequences(sequence_train, maxlen=word_num)\n",
    "x_test = pad_sequences(sequence_test, maxlen=word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels_train = df_train['label'].values\n",
    "label_encoder = LabelEncoder()\n",
    "labels_train_int = label_encoder.fit_transform(labels_train)\n",
    "labels_train = to_categorical(labels_train_int, num_classes=10)\n",
    "\n",
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into training and validation sets\n",
    "x_train, x_valid, labels_train, labels_valid = train_test_split(\n",
    "    x_train, labels_train, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for feature\n",
    "array([[   0,    0,    0, ...,    3,   61,   74],\n",
    "       [   0,    0,    0, ...,  324,  170,  238],\n",
    "       [   0,    0,    0, ...,  561, 4440,  229],\n",
    "       ...,\n",
    "       [   0,    0,    0, ...,   16,    7,  438],\n",
    "       [   0,    0,    0, ...,  340,  202,  144],\n",
    "       [   0,    0,    0, ...,  588,  448, 4507]], dtype=int32)\n",
    "\n",
    "for labels\n",
    "array([[0, 0, 0, ..., 0, 1, 0],\n",
    "       [0, 0, 0, ..., 0, 1, 0],\n",
    "       [0, 0, 0, ..., 0, 0, 0],\n",
    "       ...,\n",
    "       [1, 0, 0, ..., 0, 0, 0],\n",
    "       [0, 0, 0, ..., 0, 0, 0],\n",
    "       [0, 0, 0, ..., 1, 0, 0]])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir/lstm_bayesian_tuning_embedding_dim/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from keras_tuner import HyperModel, BayesianOptimization\n",
    "\n",
    "class LSTMHyperModel(HyperModel):\n",
    "    def __init__(self, vocabulary, word_num, output_units):\n",
    "        self.vocabulary = vocabulary\n",
    "        self.word_num = word_num\n",
    "        self.output_units = output_units\n",
    "\n",
    "    def build(self, hp):\n",
    "        \n",
    "        embedding_dim = hp.Int('embedding_dim', min_value=16, max_value=128, step=16)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.vocabulary, embedding_dim, input_length=self.word_num))\n",
    "        model.add(LSTM(units=hp.Int('units_1', min_value=32, max_value=512, step=32), return_sequences=True))\n",
    "        model.add(LSTM(units=hp.Int('units_2', min_value=16, max_value=512, step=32), return_sequences=False))\n",
    "        model.add(Dense(self.output_units, activation='softmax'))\n",
    "\n",
    "        hp_learning_rate = hp.Choice('learning_rate', values=[1e-3])\n",
    "\n",
    "        model.compile(optimizer=RMSprop(learning_rate=hp_learning_rate),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "hypermodel = LSTMHyperModel(vocabulary=10000, word_num=350, output_units=10)\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    num_initial_points=2,\n",
    "    directory='my_dir',\n",
    "    project_name='lstm_bayesian_tuning_embedding_dim'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',  \n",
    "    patience=5,              \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    x_train, labels_train,\n",
    "    epochs=10,\n",
    "    validation_data=(x_valid, labels_valid),\n",
    "    callbacks=[early_stopping]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 350, 32)           320000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 350, 320)          451840    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 240)               538560    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                2410      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1312810 (5.01 MB)\n",
      "Trainable params: 1312810 (5.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Best hyperparameters: {'embedding_dim': 32, 'units_1': 320, 'units_2': 240, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print('Best model summary:')\n",
    "best_model.summary()\n",
    "print('Best hyperparameters:', best_hyperparameters.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network using 5 fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the K-Fold Cross-Validator\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "for train, test in kfold.split(x_train, labels_train):\n",
    "    # Clone the best model\n",
    "    model = best_model  # Assuming `best_model` is already defined\n",
    "\n",
    "    # Generate a print\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = model.fit(x_train[train], labels_train[train],\n",
    "                        batch_size=32,\n",
    "                        epochs=10,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.2,  # Consider adjusting this value\n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = model.evaluate(x_train[test], labels_train[test], verbose=0)\n",
    "    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "#== Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(len(acc_per_fold)):\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "avg_loss = np.mean(loss_per_fold)\n",
    "avg_acc = np.mean(acc_per_fold)\n",
    "print(f'Average loss across all folds: {avg_loss}')\n",
    "print(f'Average accuracy across all folds: {avg_acc}%')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have separate validation data: x_val, y_val\n",
    "best_model.fit(x_train, labels_train, epochs=10, batch_size=32, \n",
    "               validation_data=(x_valid, labels_valid), \n",
    "               callbacks=[early_stopping], \n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 82s 262ms/step\n"
     ]
    }
   ],
   "source": [
    "# Now, predict x_test with the retrained model\n",
    "predictions = best_model.predict(x_test)\n",
    "\n",
    "# For multi-class classification, convert probabilities to class labels\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# If necessary, convert integer labels back to original class labels using LabelEncoder\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {\"Id\": [], \"Predicted\": []}\n",
    "for i, pred in enumerate(predicted_labels):\n",
    "    dic[\"Id\"].append(i)\n",
    "    dic[\"Predicted\"].append(pred)\n",
    "\n",
    "dic_df = pd.DataFrame.from_dict(dic)\n",
    "dic_df.to_csv(data_path + \"predicted_nn.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
